---
title: ""
sub-title: "Predicting Project Sucess"
author: "Delermando Branquinho Filho"
date: "December 18, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The Problem

Estimate Cost is a process to get an approximation of real values needed to complete the work in a project. For Project Management this process is performed periodically as needed. Looking for one solution, align over PMBoK Six Edition, we try to keep the same input and output of this process.

### The PMO controller

In this article we build a classification problem, we are using some features number of workers, the value of the tasks and time to complete the task. A label column means if the task was completed successfully from threshold restriction project.

You can insert any variable that you intended is good for your project.

### The Algorithms

We use 10 Algorithms to create a raking with the best one. We will try to have more control over the computational nuances of the training and test function, both the number of folds and the number of resampling iterations are equal to 10. Only for repeated cross-validation in k-fold: the number of complete sets of folds for compute it 3. This is more efficient than we have tried so far.

We will use 10 other algorithms besides logistic regression.

Different from split dataset in training and test, we'll to use cross validation.

Cross-validation, sometimes called rotation estimation, or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. 

### Other attempts with other algorithms

We'll try, again, Logistic Regression using this technique (k-fold) to get better results.

**Logistic regression** is a classification algorithm traditionally limited to only two-class classification problems. In this case we have no more than two classes (labels), but we'll use the Linear Discriminant Analysis to compare to linear classification technique.

**GLMNET** Extremely efficient procedures for fitting the entire lasso or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression and the Cox model.

**SVM** Radial Support Vector Machines are a famous and a very strong classification technique which does not use any sort of probabilistic model like any other classifier but simply generates hyperplanes or simply putting lines, to separate and classify the data in some feature space into different regions.

**kNN** - k-nearest neighbors algorithm In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.

**Naive Bayes** Classifier technique is based on the so-called Bayesian theorem and is particularly suited when the dimensionality of the inputs is high.

**Decision Trees** are commonly used in data mining with the objective of creating a model that predicts the value of a target (or dependent variable) based on the values of several input (or independent variables).

**CART** or Classification & Regression Trees methodology was introduced in 1984 by Leo Breiman, Jerome Friedman, Richard Olshen and Charles Stone as an umbrella term to refer to the following types of decision trees.

**Bagged CART** Bagging ensemble algorithm and the Random Forest algorithm for predictive modeling.

**Random forests** or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees

**Stochastic Gradient Boosting** (Generalized Boosted Modeling) Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.

## Step 0: Data Exploitation

In this section, we'll see what the data looks like. Look the spread data

In other hand we show you bellow a graphic we the data in 3D. It is not a good idea bacause 3D graphics, normaly, can't revel much more than numbers. If you look in detail, you'll see a little separetion in red against blue wich are the label of classes. All the data are visible mixed on , this show us that regression will fail, but let's to try to get the statistics numbers before any conclusion.

```{r init,echo=FALSE,results=F,message=F,warning=F}
library(PerformanceAnalytics,quietly = T,verbose = F,warn.conflicts = F)
library(caret,quietly = T,verbose = F,warn.conflicts = F)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"
dataset <- read.csv(file = "ProjectSucess.csv",header = T,stringsAsFactors = F,sep = ";")

head(dataset)

```

### Step 0: Data Explotation

In this section, we'll see what the data looks like.

#### Look the spread data

In other hand we show you bellow a graphic we the data in 3D.
It is not a good idea bacause 3D graphics, normaly, can't revel much more than numbers. If you look in detail, you'll see a little separetion in red against blue wich are the label of classes.
All the data are visible mixed on , this show us that regression will fail, but let's to try to get the statistics numbers before any conclusion.

```{r scatter3D}
library("scatterplot3d",quietly = T,verbose = F,warn.conflicts = F)

colors <- c("#999999", "#E69F00")
colors <- colors[as.numeric(dataset$sucess)]

scatterplot3d(x=dataset$time,y=dataset$value,z=dataset$workers, xlab = "Time", ylab = "Value", zlab = "Workers", 
          pch = 20, main = "Sucess Project", color=dataset$sucess+3)
```

#### Looking for correlation between variables

We try to see any correlation between all variable. The graphs below show us this relationship.

```{r chart_correlation}
chart.Correlation(dataset)
```

As confirmed from numbers, the correlation is high between workers to time and value. It is expected because more workers more money and more costs. About time it is inversed as expected as well. Note that you have more workers, the time to complete the task came to small.

Let's see the numbers

```{r correlation}
cor(dataset)
```

## Step 1 - Answering the first question

On this step we are using Logistic Regression model to try classification 
the dataset about *label* variable.

Like the problem ask, we try to a simple method like split dataset with 70% for train and 30% to test using glm algorithm.

```{r glm_traditional}
set.seed(seed)
split=0.70
trainIndex <- createDataPartition(dataset$sucess, p=split, list=FALSE)
data_train <- dataset[ trainIndex,]
data_test <- dataset[-trainIndex,]
# train a glm model
model <- glm(sucess ~ ., data=data_train)
# make predictions
predictions <- abs(round(predict(model, data_test)))
predictions[predictions > 1] <- 0
predictions <- as.factor(predictions)
# summarize results
```

#### summarize results

```{r conf_matrix}
        confusionMatrix(predictions, as.factor(data_test$sucess))
```

The accuracy is $0.81$, not so good because p-value from Acc > NIR is hi. We are in a 95% confidence interval, that means between $0.761$, $0.8528$. We have $167$ sucess among all project executed.

```{r conf_matrix2}
        sum(dataset$sucess==1)
```

#### Looking for Multicollinearity

In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multivariate regression model with collinear predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.

```{r Multicollinearity}
library(mctest)
omcdiag(dataset[1:3],dataset$sucess)
```

Multicollinearity Diagnostic not detected as expected.

The value and time to complete all tasks are corralated with number of workers.
We can see this in another way. Variance inflation factors measure the inflation in the variances of the parameter estimates due to collinearities that exist among the predictors. It is a measure of how much the variance of the estimated regression coefficient βk is “inflated” by the existence of correlation among the predictor variables in the model. A VIF of 1 means that there is no correlation among the kth predictor and the remaining predictor variables, and hence the variance of βk is not inflated at all. The general rule of thumb is that VIFs exceeding 4 warrant further investigation, while VIFs exceeding 10 are signs of serious multicollinearity requiring correction.

```{r Multicollinearity2}
library("olsrr")
model <- lm(sucess ~ ., data=dataset)
ols_vif_tol(model)
```

**Condition Index**

Most multivariate statistical approaches involve decomposing a correlation matrix into linear combinations of variables. The linear combinations are chosen so that the first combination has the largest possible variance (subject to some restrictions we won’t discuss), the second combination has the next largest variance, subject to being uncorrelated with the first, the third has the largest possible variance, subject to being uncorrelated with the first and second, and so forth. The variance of each of these linear combinations is called an eigenvalue. Collinearity is spotted by finding 2 or more variables that have large proportions of variance (.50 or more) that correspond to large condition indices. A rule of thumb is to label as large those condition indices in the range of 30 or larger.

```{r condition}
ols_eigen_cindex(model)
```

**Model Fit Assessment**

Residual Fit Spread Plot

Plot to detect non-linearity, influential observations and outliers. Consists of side-by-side quantile plots of the centered fit and the residuals. It shows how much variation in the data is explained by the fit and how much remains in the residuals. For inappropriate models, the spread of the residuals in such a plot is often greater than the spread of the centered fit.

```{r model_fit}
ols_plot_resid_fit_spread(model)
```

**Part & Partial Correlations**

Correlations in another way 

Relative importance of independent variables in determining Y. How much each variable uniquely contributes to R2

over and above that which can be accounted for by the other predictors.
Zero Order

Pearson correlation coefficient between the dependent variable and the independent variables.
Part

Unique contribution of independent variables. How much R2

will decrease if that variable is removed from the model?
Partial

How much of the variance in Y, which is not estimated by the other independent variables in the model, is estimated by the specific variable?

```{r corr_way2}
ols_correlations(model)
```

**Observed vs Predicted Plot**

Plot of observed vs fitted values to assess the fit of the model. Ideally, all your points should be close to a regressed diagonal line. Draw such a diagonal line within your graph and check out where the points lie. If your model had a high R Square, all the points would be close to this diagonal line. The lower the R Square, the weaker the Goodness of fit of your model, the more foggy or dispersed your points are from this diagonal line.

```{r bosxpred}
ols_plot_obs_fit(model)
```

**Diagnostics Panel**

Panel of plots for regression diagnostics

```{r lack}
ols_plot_diagnostics(model)
```


#### Residual Analysis

The difference between the observed value of the dependent variable ($y$) and the predicted value ($ŷ$) is called the residual ($e$). Each data point has one residual.

Residual = Observed value - Predicted value 
$e = y - ŷ$

Both the sum and the mean of the residuals are equal to zero. That is, $Σ e = 0$ and $e = 0$.


```{r Residuos}
par(mfrow=c(2,2))
plot(model)
```

A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate.

## Step 2

We will try to have more control over the computational nuances of the training and test function, both the number of folds and the number of resampling iterations are equal to 10. Only for repeated cross-validation in k-fold: the number of complete sets of folds for compute it 3. This is more efficient than we have tried so far.

```{r transformation_1,echo=FALSE,results=F,message=F,warning=F}
# transformation #2
# We are going to convert the label variable to factor, 
# it is easier to apply most classification 
# algorithms, because it was loaded as numeric
# after that we have two levels
dataset$sucess <- as.factor(dataset$sucess)
```


```{r algorithms_10,echo=FALSE,results=F,message=F,warning=F}
library(caret,quietly = T,verbose = F,warn.conflicts = F)
set.seed(seed)
fit.glm <- train(sucess ~., data=dataset, method="glm", metric=metric, trControl=control)
set.seed(seed)
fit.lda <- train(sucess ~., data=dataset, method="lda", 
                 metric=metric, preProc=c("center", "scale"), trControl=control)
set.seed(seed)
fit.glmnet <- train(sucess ~., data=dataset, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control)
set.seed(seed)
fit.svmRadial <- train(sucess ~., data=dataset, method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
set.seed(seed)
fit.knn <- train(sucess ~., data=dataset, method="knn", metric=metric, preProc=c("center", "scale"), trControl=control)
set.seed(seed)
fit.nb <- train(sucess ~., data=dataset, method="nb", metric=metric, trControl=control)
set.seed(seed)
fit.cart <- train(sucess ~., data=dataset, method="rpart", metric=metric, trControl=control)
set.seed(seed)
fit.treebag <- train(sucess ~., data=dataset, method="treebag", metric=metric, trControl=control)
set.seed(seed)
fit.rf <- train(sucess ~., data=dataset, method="rf", metric=metric, trControl=control)
set.seed(seed)
fit.gbm <- train(sucess ~., data=dataset, method="gbm", metric=metric, trControl=control, verbose=FALSE)
results <- resamples(list(lda=fit.lda, logistic=fit.glm, glmnet=fit.glmnet,
                          svm=fit.svmRadial, knn=fit.knn, nb=fit.nb, cart=fit.cart, 
                          bagging=fit.treebag, rf=fit.rf, gbm=fit.gbm))
```

### Final considerations

**Accuracy**

Scientists evaluate experimental results for both precision and accuracy, and in most fields, it's common to express accuracy as a percentage. 

**KAPPA**

Cohen's kappa coefficient (κ) is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, as κ takes into account the possibility of the agreement occurring by chance.

### Table comparison
```{r table_compare}
summary(results)
```

We can see in the results of the above table that the best performance of 79.87% was of the algorithm **CART**, followed by **RF **with 79.60% and 78.87 for **SVM**.

The *Kappa* value was better in **RF** with 0.574 followed by **SVM** with 0.555 and **CART** with 0.542.

Since we use the cross-validation technique, we will choose the average of 30 attempts. This leads us to believe that **RF was better**, both in percentage and Kappa, with 77.85% and 0.561 respectively.

### Boxplot comparison

In descriptive statistics, a box plot or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.

```{r boxplot_compare}
bwplot(results)
```

The boxplot chart corroborates the values mentioned in the table above with the best performance.
dotplot(results,stacked = FALSE, pch = c(19, 20), main = "Performance for all models
### Dot-plot comparison

```{r dotplot_compare}
dotplot(results,stacked = FALSE, pch = c(19, 20), main = "Performance for all models")
```

The comparison of points follows the boxplot including the variation of the minimum and maximum from the average.

### Looking for the fit about Random Forest


```{r plot_rf}
library("randomForest")

fit.rf = randomForest(sucess ~ ., data=dataset, importance=TRUE)
plot(fit.rf, main = "Plotting the Error vs Number of Trees Graph")
```


Let's to see the confusion matrix with 100% of accuracy using Random Forest.

```{r predicting}
library("randomForest")
fit.rf
```

Let us take a look at the importance that our classifier has assigned to each variable:

```{r predicting3}
library("randomForest")
varImpPlot(fit.rf, main="Importance of Variables")
```

* MeanDecreaseAccuracy: gives a rough estimate of the loss in prediction performance when that particular variable is omitted from the training set. Caveat: if two variables are somewhat redundant, then omitting one of them may not lead to massive gains in prediction performance, but would make the workers variable more important. **Value of your employees**.

* MeanDecreaseGini: GINI is a measure of node impurity. Think of it like this, if you use this feature to split the data, how pure will the nodes be? Highest purity means that each node contains only elements of a single class. Assessing the decrease in GINI when that feature is omitted leads to an understanding of how important that feature is to split the data correctly.

Do note that these measures are used to rank variables in terms of importance and, thus, their absolute values could be disregarded.


**Creating a test to see if our project will be sucess**

Now let's try to submit some random values for our model to predict if our project will have sucess or not.

The values for two projects are:
- Time 20 and 3
- Value 530 amd 400
- workers 21 and 14

```{r predicting2}
newDS = data.frame(time=c(20,3),value=c(530,400),workers=c(21,14))
as.character(predict(fit.rf,newdata = newDS))
```

The prediction, correctly, give us the first project will not be good, but the secondo will be great. 

## Predicting new value for a new Project

## Conclusion

The Random Forest (RF) algorithm for regression and classification has considerably gained popularity since its introduction in 2001. Meanwhile, it has grown to a standard classification approach competing with logistic regression in many innovation-friendly scientific fields.

In this context, we present a good scale benchmarking experiment based on one datasets comparing the prediction performance of the original version of RF with default parameters and Logistic Rregression as binary classification tools. Most importantly, the design of our benchmark experiment is inspired from clinical trial methodology, thus avoiding common pitfalls and major sources of biases.

**Random Forest** (RF) performed better than **Logistic Regression** (LR) according to the considered accuracy measured on the dataset. The mean difference between **RF** and **LR** was $0.12$ percentual points for the accuracy, and $0.43$ of Kappa, all measures thus suggesting a significantly better performance of **RF** as well as **GDM**. As a side-result of our benchmarking experiment, we observed that the results were noticeably dependent on the inclusion criteria used to select the example dataset (cross-validation against split dataset), thus emphasizing the importance of clear statements regarding this dataset selection process. We also stress that neutral studies similar to ours, based on a high number of datasets to training and test and carefully designed, will be necessary in the future to evaluate further variants, implementations or parameters of random forests which may yield improved accuracy compared to the original version with default values.


*Source*

Some explanatory texts, such as CART, RF among others were taken from the Internet (Wikipedia).